

# app/helpers/resume_helper.py
# Combines your resume AI logic + detailed ATS scoring
# Updated: robust language normalization + consistent OpenRouter usage

import json
import io
import pdfplumber
import re
from datetime import datetime
from urllib.parse import urlparse

from app.config import db, openrouter_client  # config.py defines db and openrouter_client


# -------------------------
# Helper: normalize languages
# -------------------------
import re

def normalize_languages(text):
    """Extract languages with proficiency from text or list input."""
    
    if not text:
        return []

    # If input is a list, join everything into one string
    if isinstance(text, list):
        text = " ".join(str(t) for t in text if t)

    text_lower = text.lower()

    language_patterns = [
        "english", "tamil", "hindi", "telugu", "malayalam", "kannada",
        "french", "german", "spanish", "marathi", "bengali", "punjabi",
        "gujarati", "urdu", "oriya", "nepali"
    ]

    pattern = r"\b(" + "|".join(language_patterns) + r")\b\s*(\([^)]+\))?"

    matches = re.findall(pattern, text_lower)

    results = []
    for lang, prof in matches:
        lang_clean = lang.capitalize()
        if prof:
            prof_clean = re.sub(r"\s+", "", prof.upper())
            result = f"{lang_clean} {prof_clean}"
        else:
            result = lang_clean

        if result not in results:
            results.append(result)

    return results



# -------------------------
# Extract username from URL
# -------------------------
def extract_username_from_input(input_str: str):
    if not input_str:
        return None
    s = input_str.strip()
    if s.startswith(("http://", "https://")) or "github.com" in s:
        if s.startswith("github.com"):
            s = "https://" + s
        parsed = urlparse(s)
        parts = [p for p in parsed.path.split("/") if p]
        return parts[0] if parts else None
    if re.match(r"^[A-Za-z0-9-_.]{1,39}$", s):
        return s
    if "/" in s:
        return s.split("/")[-1]
    return None


# -------------------------
# ATS Score Calculator
# -------------------------
import re

def calculate_ats_score(data, text, normalized_languages=None):
    score_details = {}

    text_lower = (text or "").lower()

    action_verbs = r"\b(developed|built|designed|implemented|managed|optimized|increased|reduced|led|collaborated|deployed|created|trained|improved|tested|analyzed|automated|integrated|streamlined|orchestrated|debugged|resolved|scaled|architected)\b"
    metrics_regex = r"\b(\d+%|\d+\s+(users|clients|projects|years|months)|\$\d+|\d+\s+(x|times))\b"

    tech_keywords = [
        "python","java","c++","c#","go","node","react","angular","vue","javascript","typescript",
        "mongodb","mysql","postgres","sql","redis",
        "aws","azure","gcp","docker","kubernetes","terraform",
        "tensorflow","pytorch","scikit-learn","pandas","numpy",
        "fastapi","django","flask","spring","express",
        "rest","graphql","microservices"
    ]

    tools_keywords = [
        "git","github","gitlab","jira","jenkins","figma","linux","bash","shell",
        "tableau","power bi","excel","visual studio","vscode","colab"
    ]

    soft_skills = ["leadership","communication","teamwork","problem solving","ownership","adaptability"]
    cert_keywords = ["certified","certificate","aws","azure","gcp","oracle","pmp","scrum","cisco"]

    # 1Ô∏è‚É£ Section coverage (max 20)
    required_sections = ["experience", "education", "skills", "projects"]
    section_score = sum(
        5 for section in required_sections if data.get(section)
    )
    score_details["Section Coverage"] = min(section_score, 20)

    # 2Ô∏è‚É£ Contact info (max 10)
    contact_score = 0
    if re.search(r"[a-z0-9._%+-]+@[a-z0-9.-]+\.\w+", text_lower): contact_score += 5
    if re.search(r"(linkedin\.com|github\.com)", text_lower): contact_score += 5
    score_details["Contact Info"] = min(contact_score, 10)

    # 3Ô∏è‚É£ Word count (max 5)
    word_count = len(text.split())
    wc_score = 5 if 500 <= word_count <= 1200 else 3 if word_count >= 300 else 0
    score_details["Word Count"] = wc_score

    # 4Ô∏è‚É£ Bullet points (max 10)
    bullet_count = len(re.findall(r"(\n\s*[-‚Ä¢*])", text))
    bp_score = 10 if bullet_count >= 10 else 5 if bullet_count >= 4 else 0
    score_details["Bullet Points"] = bp_score

    # 5Ô∏è‚É£ Action verbs + metrics (max 20)
    action_count = len(re.findall(action_verbs, text_lower))
    metric_count = len(re.findall(metrics_regex, text_lower))
    achievement_score = min(action_count*1 + metric_count*2, 20)
    score_details["Action + Achievements"] = achievement_score

    # 6Ô∏è‚É£ Technical skills + tools (max 30)
    tech_match = sum(1 for kw in tech_keywords if kw in text_lower)
    tools_match = sum(1 for kw in tools_keywords if kw in text_lower)
    skill_score = min((tech_match * 1.5) + (tools_match * 1), 30)
    score_details["Skills Strength"] = skill_score

    # 7Ô∏è‚É£ Soft skills + certifications (max 5)
    soft_score = min(sum(1 for kw in soft_skills if kw in text_lower), 2)
    cert_score = min(sum(1 for kw in cert_keywords if kw in text_lower), 3)
    score_details["Soft Skills & Certs"] = soft_score + cert_score

    # 8Ô∏è‚É£ Formatting Penalty (-5)
    penalty = 0
    if re.search(r"\.(png|jpg|jpeg|svg)", text_lower): penalty += 2
    try:
        if len(max(text.split("\n"), key=len)) > 150: penalty += 1
    except:
        pass
    score_details["Formatting Penalty"] = -min(penalty, 5)

    # Final score (cap at 100)
    total_score = sum(score_details.values())
    total_score = max(min(total_score, 100), 0)

    score_details["Total ATS Score"] = round(total_score, 2)

    return {
        "ats_breakdown": score_details,
        "ats_score": score_details["Total ATS Score"],
        "word_count": word_count,
        "languages": normalized_languages,
    }


# -------------------------
# LLM-based Certificate Evaluation
# -------------------------
async def evaluate_certificates(cert_list):
    """Evaluate certificate worthiness using OpenRouter LLM.

    Returns a list of objects with keys:
    certificate, worthiness_score (0-100 int), highlight (bool), reason (str).
    On any error or missing client, returns [].
    """
    if not openrouter_client:
        return []

    if not cert_list:
        return []

    # Ensure we work with a simple list of strings
    cert_strings = [str(c).strip() for c in cert_list if str(c).strip()]
    if not cert_strings:
        return []

    prompt = (
        "You are evaluating certificates for freshers. "
        "For each certificate, rate its industry value, difficulty, and relevance to IT/software roles. "
        "Return JSON only with the following schema (no extra text):\n\n"
        "[\n"
        "  {\n"
        "    \"certificate\": \"<original certificate string>\",\n"
        "    \"worthiness_score\": 0,\n"
        "    \"highlight\": false,\n"
        "    \"reason\": \"<short plain-text explanation>\"\n"
        "  }\n"
        "]\n\n"
        f"Certificates: {json.dumps(cert_strings)}"
    )

    try:
        response = openrouter_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "system",
                    "content": "Return JSON only. Do not include commentary or markdown fences.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            max_tokens=600,
        )
        raw = response.choices[0].message.content or ""
    except Exception as exc:
        print("‚ö†Ô∏è Certificate evaluation LLM call failed:", exc)
        return []

    raw_clean = raw.strip().replace("```json", "").replace("```", "").strip()

    try:
        parsed = json.loads(raw_clean)
    except Exception as exc:
        print("‚ö†Ô∏è Certificate evaluation JSON parse failed:", exc, "Raw:", raw_clean[:300])
        return []

    if isinstance(parsed, dict) and "results" in parsed:
        items = parsed.get("results", [])
    else:
        items = parsed

    if not isinstance(items, list):
        return []

    results = []
    for obj in items:
        if not isinstance(obj, dict):
            continue

        certificate = str(obj.get("certificate", "")).strip()
        if not certificate:
            continue

        # Coerce score to int 0-100
        score_val = obj.get("worthiness_score", 0)
        try:
            score_int = int(score_val)
        except Exception:
            score_int = 0
        score_int = max(0, min(100, score_int))

        # Coerce highlight to bool
        highlight_val = obj.get("highlight", False)
        if isinstance(highlight_val, str):
            highlight_bool = highlight_val.strip().lower() in {"true", "yes", "1"}
        else:
            highlight_bool = bool(highlight_val)

        reason = str(obj.get("reason", "")).strip()

        results.append(
            {
                "certificate": certificate,
                "worthiness_score": score_int,
                "highlight": highlight_bool,
                "reason": reason,
            }
        )

    return results


# -------------------------
# LLM-based Project Evaluation
# -------------------------
async def evaluate_projects(project_list):
    """Evaluate projects using OpenRouter LLM.

    Expects project_list as a list of strings or dicts from the resume JSON.
    Returns a list of objects with keys as per the strict schema:

    [
      {
        "project_title": "",
        "summary": "",
        "technologies": [],
        "domain": "",
        "problem_statement": "",
        "features": [],
        "impact": "",
        "complexity_level": "Beginner | Intermediate | Advanced",
        "relevance_score": 0,
        "missing_points": [],
        "recommended_improvements": [],
        "role_mapping": []
      }
    ]

    On any error, returns [].
    """
    print(f"DEBUG: Starting project evaluation. Input type: {type(project_list)}")
    print(f"DEBUG: Project list content: {project_list}")

    if not openrouter_client:
        print("DEBUG: OpenRouter client not available")
        return []

    if not project_list:
        print("DEBUG: Empty project list provided")
        return []

    # Normalize to a list of dictionaries with consistent structure
    normalized_projects = []
    for i, p in enumerate(project_list, 1):
        try:
            if p is None:
                print(f"‚ö†Ô∏è [WARN] Project {i} is None, skipping")
                continue
                
            project_dict = {
                "title": "",
                "description": "",
                "technologies": [],
                "details": []
            }
            
            if isinstance(p, dict):
                # Handle dictionary input
                project_dict["title"] = str(p.get("title") or p.get("name") or "").strip()
                project_dict["description"] = str(p.get("description") or p.get("summary") or "").strip()
                
                # Handle technologies/tech_stack
                tech = p.get("technologies") or p.get("tech_stack") or []
                if isinstance(tech, str):
                    project_dict["technologies"] = [t.strip() for t in tech.split(",") if t.strip()]
                elif isinstance(tech, list):
                    project_dict["technologies"] = [str(t).strip() for t in tech if str(t).strip()]
                
                # Handle details/description
                details = p.get("details")
                if details:
                    if isinstance(details, str):
                        project_dict["details"] = [d.strip() for d in details.split(". ") if d.strip()]
                    elif isinstance(details, list):
                        project_dict["details"] = [str(d).strip() for d in details if str(d).strip()]
                
            elif isinstance(p, str):
                # For string input, use it as description and try to extract technologies
                project_dict["description"] = p.strip()
                # Simple tech extraction - can be enhanced with more sophisticated parsing
                tech_keywords = ["python", "django", "react", "node", "java", "ml", "ai", "sql"]
                project_dict["technologies"] = [tech for tech in tech_keywords if tech in p.lower()]
            
            # Ensure we have at least a title or description
            if not project_dict["title"] and project_dict["description"]:
                # Use first few words of description as title if title is missing
                project_dict["title"] = " ".join(project_dict["description"].split()[:5]) + ("..." if len(project_dict["description"].split()) > 5 else "")
            
            print(f"‚úÖ [DEBUG] Normalized project {i}: {project_dict['title']}")
            normalized_projects.append(project_dict)
            
        except Exception as e:
            print(f"‚ö†Ô∏è [WARN] Error normalizing project {i}: {str(e)}")
            continue

    if not normalized_projects:
        return []

    # Build a more structured prompt with clear examples
    prompt = (
        "You are a senior technical recruiter evaluating student projects. For each project below, provide a detailed analysis "
        "following this JSON schema. Be thorough but concise in your evaluation.\n\n"
        "For each project, analyze and provide:\n"
        "1. A clear project title (if not provided, create one based on the description)\n"
        "2. A concise summary (2-3 sentences)\n"
        "3. List of all technologies used (programming languages, frameworks, tools)\n"
        "4. Primary domain (choose from: Web Development, AI/ML, Cloud, Full Stack, Mobile App, IoT, Cybersecurity, Data Science, Automation, Other)\n"
        "5. Problem statement (what problem does this project solve?)\n"
        "6. Key features (bullet points)\n"
        "7. Impact/Results (quantify if possible, e.g., 'Improved performance by 40%')\n"
        "8. Complexity level (Beginner, Intermediate, or Advanced)\n"
        "9. Relevance score (0-100) for fresher hiring\n"
        "10. Missing points (e.g., 'No GitHub link', 'No deployment', 'Lacks metrics')\n"
        "11. Recommended improvements\n"
        "12. Suitable job roles\n\n"
        "Example output for a project:\n"
        "{\n"
        "  \"project_title\": \"E-commerce Website\",\n"
        "  \"summary\": \"Developed a full-stack e-commerce platform with user authentication and payment integration.\",\n"
        "  \"technologies\": [\"React\", \"Node.js\", \"MongoDB\", \"Express\"],\n"
        "  \"domain\": \"Web Development\",\n"
        "  \"problem_statement\": \"Small businesses need an affordable way to sell products online.\",\n"
        "  \"features\": [\"User authentication\", \"Product catalog\", \"Shopping cart\", \"Payment processing\"],\n"
        "  \"impact\": \"Enabled small businesses to go online, resulting in 30% increased sales.\",\n"
        "  \"complexity_level\": \"Intermediate\",\n"
        "  \"relevance_score\": 85,\n"
        "  \"missing_points\": [\"No link to live demo\", \"Lacks test coverage\"],\n"
        "  \"recommended_improvements\": [\"Add unit tests\", \"Implement CI/CD\", \"Add responsive design for mobile\"],\n"
        "  \"role_mapping\": [\"Full Stack Developer\", \"Frontend Developer\", \"Web Developer\"]\n"
        "}\n\n"
        "Now evaluate these projects (maintain the same order):\n"
        f"{json.dumps(normalized_projects, indent=2)}\n\n"
        "Return a JSON array of your evaluations. Do not include any other text or markdown formatting."
    )

    try:
        print("ü§ñ [DEBUG] Sending project evaluation request to OpenRouter...")
        print(f"üìù [DEBUG] Prompt length: {len(prompt)} characters")
        
        # Make the API call
        response = openrouter_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant that evaluates technical projects. Return a valid JSON array of project evaluations.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            max_tokens=3000,  # Increased to handle larger responses
            response_format={"type": "json_object"},
            timeout=30  # 30 second timeout
        )
        
        raw = response.choices[0].message.content or ""
        print(f"‚úÖ [DEBUG] Received response from OpenRouter (length: {len(raw)})")
        
        # Clean the response
        raw_clean = raw.strip()
        
        # Try to extract JSON if it's wrapped in markdown code blocks
        if "```json" in raw_clean:
            raw_clean = raw_clean.split("```json")[1].split("```")[0].strip()
        elif "```" in raw_clean:
            raw_clean = raw_clean.split("```")[1].split("```")[0].strip()
        
        print(f"üîç [DEBUG] Raw LLM response (first 500 chars): {raw_clean[:500]}...")
        
        # Parse the JSON response
        parsed = json.loads(raw_clean)
        
        # Handle different response formats
        if isinstance(parsed, dict):
            if "projects" in parsed:
                items = parsed["projects"]
            elif "results" in parsed:
                items = parsed["results"]
            elif any(k in parsed for k in ["project_title", "summary", "technologies"]):
                # If it's a single project object, wrap it in a list
                items = [parsed]
            else:
                # Try to find an array in the response
                items = next((v for v in parsed.values() if isinstance(v, list)), [])
        elif isinstance(parsed, list):
            items = parsed
        else:
            raise ValueError(f"Unexpected response format: {type(parsed).__name__}")
            
        if not isinstance(items, list):
            raise ValueError(f"Expected list of projects, got {type(items).__name__}")
            
        print(f"‚úÖ [DEBUG] Successfully parsed {len(items)} projects")
        
    except json.JSONDecodeError as je:
        print(f"‚ùå [ERROR] Failed to parse JSON from LLM: {str(je)}")
        print(f"üìÑ [DEBUG] Raw response start: {raw_clean[:500]}...")
        if len(raw_clean) > 500:
            print(f"üìÑ [DEBUG] Raw response end: ...{raw_clean[-500:]}")
        return []
    except Exception as exc:
        print(f"‚ùå [ERROR] Project evaluation failed: {str(exc)}")
        import traceback
        print(f"üìú Stack trace: {traceback.format_exc()}")
        return []

    # Ensure we have a valid list of projects
    if not isinstance(items, list):
        print(f"‚ö†Ô∏è [WARN] Expected list of projects, got {type(items).__name__}")
        return []

    allowed_domains = {
        "web development",
        "ai/ml",
        "cloud",
        "full stack",
        "mobile app",
        "iot",
        "cybersecurity",
        "data science",
        "automation",
        "other",
    }

    valid_complexities = {"beginner", "intermediate", "advanced"}

    results = []
    for obj in items:
        if not isinstance(obj, dict):
            continue

        title = str(obj.get("project_title", "")).strip()
        summary = str(obj.get("summary", "")).strip()
        problem = str(obj.get("problem_statement", "")).strip()
        impact = str(obj.get("impact", "")).strip()

        # Skip completely empty entries
        if not (title or summary or problem or impact):
            continue

        techs = obj.get("technologies", [])
        if isinstance(techs, list):
            techs_clean = [str(t).strip() for t in techs if str(t).strip()]
        elif isinstance(techs, str):
            techs_clean = [s.strip() for s in techs.split(",") if s.strip()]
        else:
            techs_clean = []

        domain_val = str(obj.get("domain", "")).strip()
        domain_lower = domain_val.lower()
        if domain_lower not in allowed_domains:
            domain_val = "Other"

        complexity_val = str(obj.get("complexity_level", "")).strip()
        comp_lower = complexity_val.lower()
        if comp_lower not in valid_complexities:
            # Try to infer from wording if missing/invalid
            comp_lower = "intermediate"
        # Normalize capitalization
        if comp_lower == "beginner":
            complexity_val = "Beginner"
        elif comp_lower == "advanced":
            complexity_val = "Advanced"
        else:
            complexity_val = "Intermediate"

        score_val = obj.get("relevance_score", 0)
        try:
            score_int = int(score_val)
        except Exception:
            score_int = 0
        score_int = max(0, min(100, score_int))

        def to_str_list(value):
            if not value:
                return []
            if isinstance(value, list):
                return [str(v).strip() for v in value if str(v).strip()]
            return [str(value).strip()]

        features = to_str_list(obj.get("features"))
        missing_points = to_str_list(obj.get("missing_points"))
        improvements = to_str_list(obj.get("recommended_improvements"))
        role_mapping = to_str_list(obj.get("role_mapping"))

        results.append(
            {
                "project_title": title,
                "summary": summary,
                "technologies": techs_clean,
                "domain": domain_val,
                "problem_statement": problem,
                "features": features,
                "impact": impact,
                "complexity_level": complexity_val,
                "relevance_score": score_int,
                "missing_points": missing_points,
                "recommended_improvements": improvements,
                "role_mapping": role_mapping,
            }
        )

    return results


# -------------------------
# Core Resume Processor
# -------------------------
async def process_resume_file(upload_file):
    """Handle resume PDF upload + AI parsing + ATS scoring + DB save."""
    try:
        contents = await upload_file.read()
        if not contents:
            return {"error": "Empty file received. Please upload a valid PDF."}

        with pdfplumber.open(io.BytesIO(contents)) as pdf:
            text = "\n".join(page.extract_text() or "" for page in pdf.pages)

        if not text.strip():
            return {"error": "No readable text found in the uploaded PDF."}

        if not openrouter_client:
            return {"error": "AI client not configured (missing OPENROUTER_API_KEY)."}

        # Truncate long documents for model context safety (you already used similar earlier)
        max_chars = 80000  # conservative
        prompt_text = text if len(text) <= max_chars else text[:max_chars] + "\n...[Truncated for AI]..."

        # ---- AI extraction ----
        prompt = f"""
Extract structured resume info and return valid JSON ONLY.

CRITICAL RULE FOR CERTIFICATES:
  The "certificates" array must be extracted ONLY from sections whose heading (case-insensitive)
  matches one of the following variations. Ignore any certificate-like text outside these sections.
  Allowed headings:
    ["CERTIFICATE", "CERTIFICATES", "CERTIFICATION", "CERTIFICATIONS",
     "COURSES", "TRAININGS", "ACHIEVEMENTS", "SKILL CERTIFICATIONS",
     "ONLINE COURSES", "LICENSES", "CREDENTIALS"].

Return JSON in this exact structure:
{{
  "name": "", 
  "email": "", 
  "phone": "",
  "linkedin": "", 
  "github": "", 
  "leetcode": "", 
  "codechef": "",
  "languages": [],  
  "education": {{
      "10th": {{
          "school": "",
          "location": "",
          "year": "",
          "percentage": ""
      }},
      "12th": {{
          "school": "",
          "location": "",
          "year": "",
          "percentage": ""
      }},
      "bachelor": {{
          "institute": "",
          "location": "",
          "degree": "",
          "expected_graduation": "",
          "cgpa": ""
      }}
  }},
  "skills": {{
      "technical": [],
      "soft": [],
      "area_of_interest": []
  }},
  "internships": [],
  "projects": [],
  "certificates": [], 
  "role_match": "", 
  "summary": ""
}}

Resume text:
{prompt_text}
"""

        # ---- Call AI model via openrouter_client ----
        try:
            response = openrouter_client.chat.completions.create(
                model="gpt-4.1-mini",
                messages=[
                    {"role": "system", "content": "Return valid JSON only. Do not include commentary."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                # Slightly reduced to stay within current OpenRouter credit max_tokens limit
                max_tokens=1500,
            )
            ai_output = response.choices[0].message.content
            print(f"DEBUG: Raw AI output: {ai_output}")  # Debug log for raw AI output
        except Exception as exc:
            print(f"DEBUG: AI request failed: {str(exc)}")
            return {"error": f"AI request failed: {str(exc)}"}

        # ---- Parse JSON ----
        ai_output = ai_output.strip().replace("```json", "").replace("```", "")
        try:
            data = json.loads(ai_output)
        except Exception as e:
            return {"error": f"Failed to parse AI JSON: {str(e)}", "raw": ai_output}

        # ---- Certificate worthiness evaluation (non-blocking fallback on failure) ----
        cert_list = data.get("certificates", []) or []
        cert_analysis = await evaluate_certificates(cert_list)
        if not isinstance(cert_analysis, list):
            cert_analysis = []
        data["certificate_analysis"] = cert_analysis

        # Process projects if any
        projects = []
        if "projects" in data and data["projects"]:
            print(f"DEBUG: Found projects in resume data: {data['projects']}")  # Debug log
            projects = await evaluate_projects(data["projects"])
            print(f"DEBUG: Evaluated projects: {projects}")  # Debug log
        else:
            print("DEBUG: No projects found in resume data")  # Debug log
        data["project_analysis"] = projects

        # ---- Technical skills post-processing ----
        # Clean the skills block minimally so that:
        # 1) Items listed under area_of_interest are NOT duplicated in technical skills
        # 2) Long strings that match certificate titles are NOT treated as technical skills
        skills_block = data.get("skills", {}) or {}

        technical_skills = skills_block.get("technical") or []
        aoi_list = skills_block.get("area_of_interest") or []
        certificates_list = data.get("certificates") or []

        # Normalise to lists of strings
        if isinstance(technical_skills, str):
            technical_skills = [s.strip() for s in re.split(r"[,;/]\s*", technical_skills) if s.strip()]
        if isinstance(aoi_list, str):
            aoi_list = [s.strip() for s in re.split(r"[,;/]\s*", aoi_list) if s.strip()]
        if isinstance(certificates_list, str):
            certificates_list = [certificates_list]

        aoi_lower = {str(x).strip().lower() for x in aoi_list if str(x).strip()}
        cert_lower_list = [str(c).strip().lower() for c in certificates_list if str(c).strip()]

        cleaned_technical = []
        seen = set()
        for item in technical_skills:
            s = str(item).strip()
            if not s:
                continue

            key = s.lower()

            # Skip if this exact text already exists under Areas of Interest
            if key in aoi_lower:
                continue

            # Skip if this matches or is a clear substring of any certificate title.
            # This catches both full names and shorter provider phrases like
            # "microsoft azure" or "google cloud" when they appear in
            # certificate strings such as "microsoft azure az-900".
            for c in cert_lower_list:
                # only treat as certificate if the certificate string is longer
                if len(c.split()) >= len(key.split()):
                    if key == c or key in c or c in key:
                        key = None
                        break
            if key is None:
                continue

            if key not in seen:
                seen.add(key)
                cleaned_technical.append(s)

        skills_block["technical"] = cleaned_technical
        data["skills"] = skills_block

        # ---- Normalize languages (robust, but no guessing) ----
        # Use only what the AI explicitly returns in the languages field.
        raw_langs = data.get("languages", [])
        if isinstance(raw_langs, str):
            raw_langs = re.split(r"[,;/]| and ", raw_langs)
        langs = normalize_languages([lang.strip() for lang in raw_langs if lang and str(lang).strip()])

        # If the resume does not contain a languages section, leave this empty.
        # Do NOT guess or default to English/Tamil/etc.
        data["languages"] = langs

        # ---- Compute ATS ----
        ats = calculate_ats_score(data, text, normalized_languages=langs)

        # ---- Save to MongoDB ----
        try:
            await db.reports.insert_one({
                "filename": getattr(upload_file, "filename", "uploaded_resume"),
                "data": data,
                "ats_breakdown": ats["ats_breakdown"],
                "ats_score": ats["ats_score"],
                "word_count": ats["word_count"],
                "uploaded_at": datetime.utcnow(),
            })
        except Exception as e:
            print("‚ö†Ô∏è MongoDB insert failed:", e)

        # ---- Return Result ----
        return {
            "data": data,
            "ats_score": ats["ats_score"],
            "ats_breakdown": ats["ats_breakdown"],
            "word_count": ats["word_count"],
        }

    except Exception as e:
        import traceback
        print("‚ùå Error in process_resume_file:", traceback.format_exc())
        return {"error": str(e)}


# -------------------------
# Text-only extractor (use when you pass resume text instead of PDF)
# -------------------------
async def extract_resume_data(text: str):
    """
    Extract structured resume data using OpenRouter AI.
    Returns parsed JSON with keys like name, email, education, etc.
    """
    if not openrouter_client:
        return {}


    prompt = f"""
Extract structured resume info and return valid JSON ONLY.

CRITICAL RULE FOR CERTIFICATES:
  The "certificates" array must be extracted ONLY from sections whose heading (case-insensitive)
  matches one of the following variations. Ignore any certificate-like text outside these sections.
  Allowed headings:
    ["CERTIFICATE", "CERTIFICATES", "CERTIFICATION", "CERTIFICATIONS",
     "COURSES", "TRAININGS", "ACHIEVEMENTS", "SKILL CERTIFICATIONS",
     "ONLINE COURSES", "LICENSES", "CREDENTIALS"].

Return JSON in this exact structure:
{{
  "name": "", "email": "", "phone": "",
  "linkedin": "", "github": "", "leetcode": "", "codechef": "",
  "languages": [],
  "education": {{
      "10th": {{"school": "", "location": "", "year": "", "percentage": ""}},
      "12th": {{"school": "", "location": "", "year": "", "percentage": ""}},
      "bachelor": {{"institute": "", "location": "", "degree": "", "expected_graduation": "", "cgpa": ""}}
  }},
  "skills": {{"technical": [], "soft": []}},
  "certificates": [],
  "role_match": "",
  "summary": ""
}}

Resume text:
{text}
"""
    try:
        completion = openrouter_client.chat.completions.create(
            model="gpt-4.1-mini",
            messages=[{"role": "system", "content": "Return valid JSON only."}, {"role": "user", "content": prompt}],
            temperature=0.2,
        )
        raw = completion.choices[0].message.content.strip()
    except Exception as exc:
        print("‚ö†Ô∏è OpenRouter request failed in extract_resume_data:", exc)
        return {}

    raw_clean = raw.replace("```json", "").replace("```", "").strip()
    try:
        data = json.loads(raw_clean)
    except Exception:
        print("‚ö†Ô∏è Invalid JSON returned by AI (extract_resume_data):", raw_clean)
        return {}

    # Normalize languages same as process function, without guessing
    raw_langs = data.get("languages", [])
    if isinstance(raw_langs, str):
        raw_langs = re.split(r"[,;/]| and ", raw_langs)
    langs = normalize_languages([lang.strip() for lang in raw_langs if lang and str(lang).strip()])

    # If AI did not return languages, leave this empty instead of guessing.
    data["languages"] = langs
    return data


# ---------------------------------------------------------
# STRICT MODE ‚Äì Extract ONLY when headings exist
# ---------------------------------------------------------

TECH_SKILL_HEADINGS = [
    "technical skills","technical skill","skills","skillset","tech skills",
    "core skills","key skills","hard skills","professional skills",
    "technical proficiencies","technical proficiency","technical expertise",
    "software skills","tools & technologies","tools and technologies",
    "technologies","tech stack","technical toolkit","programming skills",
    "programming languages","programming language",
    "it skills","computer skills","domain skills","specialized skills",
    "primary skills","relevant skills","development skills",
    "technical competencies","skill highlights","skills highlights",
    "languages","language","Languages","Language",
    "tools","tool","technical tools","software tools","developer tools",
]

AREA_INTEREST_HEADINGS = [
    "area of interest","areas of interest","interests","interest areas",
    "technical interests","professional interests","career interests",
    "domain interests","engineering interests","fields of interest",
    "preferred domains","preferred areas","preferred technical areas",
    "specialization areas","preferred fields","passion","my interests"
]


def extract_text_from_pdf(file_bytes: bytes):
    try:
        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
            text = "\n".join([page.extract_text() or "" for page in pdf.pages])
        return text
    except Exception as e:
        print("PDF EXTRACT ERROR:", e)
        return ""


def normalize_text(text: str) -> str:
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def heading_exists(text: str, heading_list: list):
    text_lower = text.lower()
    for h in heading_list:
        if h in text_lower:
            return True
    return False


def build_resume_prompt(extracted_text: str, found_skill_heading: bool, found_interest_heading: bool):
    return f"""
### STRICT EXTRACTION MODE

Below is RAW resume text. You MUST follow all strict rules:

RAW RESUME TEXT:
{extracted_text}

------------------------------------------------
RULES:
------------------------------------------------

1. You must extract ONLY from the correct heading section.

   Technical Skills:
   - Extract ONLY if a valid technical-skills heading exists.
   - Treat headings like: "TECHNICAL SKILLS", "TECHNICAL SKILL", "SKILLS", "TOOLS", "SOFTWARE TOOLS", "PROGRAMMING LANGUAGES" as technical-skill sections.
   - Extract ONLY items listed under those headings.
   - DO NOT extract technical skills from summary, experience, projects, CERTIFICATIONS, COURSES, or AOI sections.

   Areas of Interest:
   - Extract ONLY if a valid area-of-interest heading exists.
   - Extract ONLY items listed under that heading.
   - DO NOT extract AOI from summary, career objective, projects, experience, skills section.

2. TECH-SKILL VS AOI SEPARATION RULE:
   - If an item appears under AOI heading ‚Üí treat as AOI (even if technical)
   - If an item appears under TECH-SKILL heading ‚Üí treat as a technical skill
   - DO NOT mix the two categories
   - DO NOT infer missing items

3. If heading is NOT found:
   - Return "not found" for that section.
   - DO NOT guess or infer.

------------------------------------------------
EXPECTED OUTPUT FORMAT:
{{
  "technical_skills": [... OR "not found"],
  "areas_of_interest": [... OR "not found"]
}}
------------------------------------------------

Extract now.
"""


def llm_extract_resume_details(raw_text: str, found_skill_heading: bool, found_interest_heading: bool):
    prompt = build_resume_prompt(raw_text, found_skill_heading, found_interest_heading)

    response = openrouter_client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
        max_tokens=600,
    )

    content = (
        response.choices[0].message["content"]
        if isinstance(response.choices[0].message, dict)
        else response.choices[0].message.content
    )

    try:
        parsed = json.loads(content)
    except Exception:
        match = re.search(r"\{.*\}", content, re.DOTALL)
        parsed = json.loads(match.group(0)) if match else {}

    tech = parsed.get("technical_skills", "not found")
    aoi = parsed.get("areas_of_interest", "not found")

    # return clean strict output
    return {
        "technical_skills": tech,
        "areas_of_interest": aoi
    }


def extract_resume_details(file_bytes: bytes):
    raw = extract_text_from_pdf(file_bytes)
    raw = normalize_text(raw)

    found_skill_heading = heading_exists(raw, TECH_SKILL_HEADINGS)
    found_interest_heading = heading_exists(raw, AREA_INTEREST_HEADINGS)

    return llm_extract_resume_details(raw, found_skill_heading, found_interest_heading)


# ---------------------------------------------------------
# PROGRAMMING LANGUAGES MASTER LIST
# ---------------------------------------------------------
PROGRAMMING_LANGUAGES = {
    "c": ["c language", "c lang"],
    "c++": ["cpp", "c plus plus"],
    "java": ["core java", "advanced java"],
    "python": ["python programming"],
    "javascript": ["js", "nodejs", "node js"],
    "typescript": [],
    "ruby": [],
    "go": ["golang"],
    "swift": [],
    "kotlin": [],
    "php": ["php language"],
    "r": ["r language"],
    "matlab": [],
    "scala": [],
    "perl": [],
}


PROGRAMMING_SYNONYMS = {}
for canonical, aliases in PROGRAMMING_LANGUAGES.items():
    PROGRAMMING_SYNONYMS[canonical] = canonical
    for al in aliases:
        PROGRAMMING_SYNONYMS[al] = canonical


# ---------------------------------------------------------
# PROGRAMMING LANGUAGE EXTRACTOR
# ---------------------------------------------------------
def extract_programming_languages(text):
    text = text.lower()
    found = set()

    # 1. Direct canonical match
    for lang in PROGRAMMING_LANGUAGES.keys():
        if lang in text:
            found.add(lang.capitalize())

    # 2. Synonym match
    for syn, canonical in PROGRAMMING_SYNONYMS.items():
        if syn in text:
            found.add(canonical.capitalize())

    # 3. Pattern-based extraction
    pat = r"\b(c|c\+\+|java|python|javascript|typescript|php|r|go|ruby|swift|kotlin|scala|perl)\b"
    matches = re.findall(pat, text)
    for m in matches:
        found.add(m.capitalize())

    # 4. Fix ambiguous "R"
    cleaned = []
    for item in found:
        if item == "R":
            if "r " not in text and " r," not in text and "language" not in text:
                continue
        cleaned.append(item)

    return sorted(list(set(cleaned)))


# ---------------------------------------------------------
# IMPROVED TECH SKILL EXTRACTOR
# ---------------------------------------------------------
SKILL_SYNONYMS = {
    "excel": ["ms excel", "microsoft excel"],
    "power bi": ["powerbi"],
    "mysql": ["my sql", "my-sql"],
    "html": ["html5"],
    "css": ["css3"],
    "react": ["reactjs", "react js"],
    "node": ["nodejs", "node js"],
    "git": ["git version control"],
    "vscode": ["visual studio code", "vs code"],
    "jupyter": ["jupyter notebook", "jupyter lab"],
    "postman": [],
    "figma": [],
    "docker": [],
    "kubernetes": ["k8s"],
    "aws": ["amazon web services"],
    "azure": ["microsoft azure"],
    "gcp": ["google cloud platform", "google cloud"],
    "jenkins": [],
    "ansible": [],
    "terraform": [],
    "postgresql": ["postgres"],
    "mongodb": [],
    "redis": [],
    "kafka": ["apache kafka"],
    "hadoop": ["apache hadoop"],
    "spark": ["apache spark"],
    "tensorflow": [],
    "pytorch": ["pytorch"],
    "tableau": [],
    "powerbi": ["power bi"],
    "sap": [],
    "salesforce": [],
    "oracle": ["oracle db", "oracle database"],
    "mssql": ["microsoft sql server", "sql server"],
    "linux": [],
    "unix": [],
    "bash": ["bash scripting", "shell scripting"],
    "powershell": [],
    "rhel": ["red hat enterprise linux"],
    "ubuntu": [],
    "centos": [],
    "wordpress": [],
    "drupal": [],
    "magento": [],
    "shopify": [],
    "woocommerce": ["woo commerce"],
    "seo": ["search engine optimization"],
    "adobe photoshop": ["photoshop", "ps"],
    "adobe illustrator": ["illustrator", "ai"],
    "adobe xd": ["xd"],
    "sketch": [],
    "invision": [],
    "zeplin": [],
    "jira": [],
    "confluence": [],
    "trello": [],
    "asana": [],
    "slack": [],
    "zoom": [],
    "teams": ["microsoft teams"],
    "skype": [],
    "webex": ["cisco webex"],
    "vmware": [],
    "virtualbox": [],
    "wireshark": [],
    "ethernet": [],
    "tcp/ip": ["tcp", "ip"],
    "dns": ["domain name system"],
    "dhcp": ["dynamic host configuration protocol"],
    "vpn": ["virtual private network"],
    "ldap": ["lightweight directory access protocol"],
    "active directory": ["ad"],
    "oauth": ["oath 2.0"],
    "jwt": ["json web token"],
    "rest": ["restful", "rest api"],
    "graphql": ["graph ql"],
    "soap": [],
    "grpc": ["grpc"],
    "swagger": ["openapi"],
    "postman": [],
    "insomnia": [],
    "junit": [],
    "testng": [],
    "selenium": [],
    "cypress": [],
    "jest": [],
    "mocha": [],
    "jasmine": [],
    "pytest": [],
    "unittest": ["python unittest"],
    "gitlab": ["gitlab ci/cd"],
    "github": ["github actions"],
    "bitbucket": [],
    "jenkins": [],
    "travis": ["travis ci"],
    "circleci": ["circle ci"],
    "ansible": [],
    "puppet": [],
    "chef": [],
    "terraform": [],
    "kubernetes": ["k8s"],
    "docker": [],
    "docker compose": ["docker-compose"],
    "kubernetes": ["k8s"],
    "helm": [],
    "istio": [],
    "prometheus": [],
    "grafana": [],
    "kibana": [],
    "elasticsearch": ["elastic search"],
    "logstash": [],
    "filebeat": [],
    "kafka": ["apache kafka"],
    "rabbitmq": ["rabbit mq"],
    "redis": [],
    "memcached": ["memcache"],
    "nginx": ["engine x"],
    "apache": ["apache http server"],
    "iis": ["internet information services"],
    "tomcat": ["apache tomcat"],
    "jboss": [],
    "weblogic": ["oracle weblogic server"],
    "websphere": ["ibm websphere"],
    "aws": ["amazon web services"],
    "ec2": ["amazon ec2"],
    "s3": ["amazon s3"],
    "lambda": ["aws lambda"],
    "azure": ["microsoft azure"],
    "gcp": ["google cloud platform"],
    "firebase": ["google firebase"],
    "heroku": [],
    "digitalocean": ["digital ocean"],
    "linode": [],
    "vultr": [],
    "ovh": [],
    "alibaba cloud": ["alibaba cloud", "alibabacloud"],
    "oracle cloud": ["oracle cloud infrastructure", "oci"],
    "ibm cloud": ["ibm cloud", "bluemix"],
    "salesforce": [],
    "sap": [],
    "dynamodb": ["amazon dynamodb"],
    "cassandra": ["apache cassandra"],
    "couchbase": [],
    "couchdb": ["apache couchdb"],
    "hbase": ["apache hbase"],
    "hive": ["apache hive"],
    "pig": ["apache pig"],
    "hadoop": ["apache hadoop"],
    "spark": ["apache spark"],
    "flink": ["apache flink"],
    "storm": ["apache storm"],
    "kafka": ["apache kafka"],
    "pulsar": ["apache pulsar"],
    "ignite": ["apache ignite"],
    "geode": ["apache geode"],
    "cassandra": ["apache cassandra"],
    "couchdb": ["apache couchdb"],
    "hbase": ["apache hbase"],
    "hive": ["apache hive"],
    "pig": ["apache pig"],
    "hadoop": ["apache hadoop"],
    "spark": ["apache spark"],
    "flink": ["apache flink"],
    "storm": ["apache storm"],
    "kafka": ["apache kafka"],
    "pulsar": ["apache pulsar"],
    "ignite": ["apache ignite"],
    "geode": ["apache geode"]
}


def normalize_skill(skill):
    s = skill.lower().strip()

    # Replace synonyms
    for canonical, aliases in SKILL_SYNONYMS.items():
        if s == canonical or s in aliases:
            return canonical

    return s


def extract_technical_skills(text):
    # Convert to lowercase for case-insensitive matching
    text_lower = text.lower()
    
    # Look for common section headers, including "Tools And Technology" variations
    section_headers = [
        r'(?:tools(?:\s+and\s+technolog(?:y|ies))?)',  # Matches 'Tools And Technology', 'Tools and Technologies', etc.
        r'(?:technical(?:\s+skills?|\s+stack)?)',
        r'(?:technologies?)',
        r'(?:skills?)',
        r'(?:programming(?:\s+skills?|\s+languages?)?)',
        r'(?:frameworks?)',
        r'(?:libraries?)',
        r'(?:databases?)',
        r'(?:platforms?)',
        r'(?:software(?:\s+skills?)?)',
        r'(?:technical(?:\s+expertise|\s+proficiency)?)',
        r'(?:technologies?(?:\s+and\s+tools)?)',
        r'(?:tools(?:\s+and\s+software)?)',
        r'(?:technical(?:\s+knowledge|\s+background))',
        r'(?:programming(?:\s+and\s+scripting)?)',
        r'(?:development(?:\s+environment|\s+stack)?)',
        r'(?:technical(?:\s+stack)?)',
        r'(?:it(?:\s+skills|\s+proficiency)?)',
        r'(?:computer(?:\s+skills|\s+proficiency)?)',
        r'(?:software(?:\s+proficiency|\s+skills)?)',
        r'(?:technical(?:\s+competencies|\s+abilities))'
    ]
    
    # Create a pattern that matches any of the section headers followed by content until next section or end
    section_pattern = r'(?:^|\n)\s*(' + '|'.join(section_headers) + r')\s*[\n:]+(.*?)(?=\n\s*(?:' + '|'.join(section_headers) + r'|$))'
    
    # Extract all potential skills
    all_skills = set()
    
    # Find all sections that match our pattern
    section_matches = re.finditer(section_pattern, text_lower, re.DOTALL | re.IGNORECASE)
    
    # Process each matched section
    for match in section_matches:
        section_text = match.group(2)  # Get the content after the section header
        
        # Split by common delimiters (comma, newline, semicolon, pipe, forward slash, bullet points)
        parts = re.split(r'[,\n;|/‚Ä¢¬∑]', section_text)
        
        for part in parts:
            part = part.strip()
            if not part:
                continue
                
            # Remove any trailing punctuation and special characters
            part = re.sub(r'[^\w\s-]', ' ', part)
            part = ' '.join(part.split())  # Normalize multiple spaces to single space
            part = part.strip()
            
            if part and len(part) > 1:  # Skip single characters
                # Handle version numbers (e.g., "Python 3.8" -> "python")
                base_skill = re.sub(r'\s*\d+(\.\d+)*\b', '', part).strip()
                if base_skill:
                    normalized = normalize_skill(base_skill)
                    if normalized:  # Only add if normalization returned a non-empty string
                        all_skills.add(normalized)
    
    # Also process the entire text as before for backward compatibility
    text = text.replace("|", ",")
    parts = re.split(r"[,/;\n‚Ä¢¬∑]", text)

    for p in parts:
        p = p.strip()
        if not p:
            continue
            
        # Remove any trailing punctuation and special characters
        p = re.sub(r'[^\w\s-]', ' ', p)
        p = ' '.join(p.split())  # Normalize multiple spaces to single space
        p = p.strip()
        
        if p and len(p) > 1:  # Skip single characters
            # Handle version numbers (e.g., "Python 3.8" -> "python")
            base_skill = re.sub(r'\s*\d+(\.\d+)*\b', '', p).strip()
            if base_skill:
                normalized = normalize_skill(base_skill)
                if normalized:  # Only add if normalization returned a non-empty string
                    all_skills.add(normalized)
    
    # Convert to list, capitalize, and sort
    clean = [s.capitalize() for s in all_skills if s and len(s) > 1]  # Ensure minimum 2 characters
    clean = list(dict.fromkeys(clean))  # Remove duplicates while preserving order
    clean.sort()  # Sort alphabetically
    return sorted(list(set(clean)))


# ---------------------------------------------------------
# FINAL SKILL MERGER: PROGRAMMING ‚Üí TECHNICAL
# ---------------------------------------------------------
def merge_programming_into_technical(prog, tech):
    merged = set(tech)

    for lang in prog:
        if lang not in merged:
            merged.add(lang)

    return sorted(list(merged))


# ---------------------------------------------------------
# IMPROVED ATS SCORING (LIGHTWEIGHT)
# ---------------------------------------------------------
def compute_ats_score(skills, programming_languages, area_of_interest):
    score = 0

    # More weight to programming languages
    score += len(programming_languages) * 2

    # Technical skills
    score += len(skills) * 1

    # AOI helps
    score += len(area_of_interest) * 1

    # Cap at 100
    return min(score, 100)


# ---------------------------------------------------------
# MAIN RESUME PARSER ENTRY (TEXT-ONLY, LIGHTWEIGHT)
# ---------------------------------------------------------
def parse_resume_sections(raw_text):
    raw_text = raw_text.lower()

    programming_languages = extract_programming_languages(raw_text)
    technical_skills = extract_technical_skills(raw_text)

    technical_skills_final = merge_programming_into_technical(
        programming_languages, technical_skills
    )

    area_of_interest = []

    ats_score = compute_ats_score(
        technical_skills_final, programming_languages, area_of_interest
    )

    return {
        "skills": {
            "technical": technical_skills_final,
            "programming_languages": programming_languages,
            "area_of_interest": area_of_interest,
        },
        "ats_score": ats_score,
    }
